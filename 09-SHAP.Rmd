# Mô hình Machine learning khả diễn

## Giới thiệu

...

## Bối cảnh của thí nghiệm

...

```{r,message = FALSE,warning=FALSE}
# Thao tác dữ liệu và đồ họa
library(tidyverse)

# Machine learning
library(tidymodels)
library(SHAPforxgboost)

# Đồ họa
library(lvplot)
library(GGally)
library(patchwork)
```

...

```{r}
df = read.csv('Blastocyte_quality.csv', 
              sep = ';', dec = ',', 
              fileEncoding = 'UTF-8-BOM')%>%na.omit()

names(df)
```

## Kế hoạch phân tích: 

...

## Chuẩn bị và thăm dò dữ liệu

...

```{r}
df$Blast_ICM = as.factor(df$Blast_ICM)
df$Blast_TE = as.factor(df$Blast_TE)

df$Prim_Inf %<>% as.factor()%>%
  recode_factor(.,`0` = "Primary",
                `1` = "Secondary")

df$Trigger%<>% as.factor()%>%
  recode_factor(.,`a` = "Agonist", 
                `h` = "hCG",
                `d` = "Dual")

df$ClinPreg = as.factor(df$ClinPreg)
```

...

```{r}
str(df)
```

...

### Các thông số định tính, rời rạc

...

```{r}
class_pals = c('#fc0341','#03adfc')

df%>%mutate(Day = factor(.$Day_BVitrif),
            Size = factor(.$Blast_size),
            Size_chang = factor(.$dSize),
            ICM_chang = factor(.$dICM),
            TE_chang = factor(.$Blast_TE))%>%
  gather(Day,Size,Blast_TE,Blast_ICM,
         Size_chang,ICM_chang,TE_chang,
         Prim_Inf,Trigger,
            key="Feature",
            value="Status")%>%
  ggplot(aes(x=Status,
             fill=ClinPreg))+
  geom_bar(stat="count",
           position="fill",
           alpha=0.5,
           col="black")+
  coord_flip()+
  scale_y_continuous(labels=NULL)+
  theme_bw()+
  facet_wrap(~Feature,scales="free",ncol=3)+
  scale_fill_manual(values=class_pals)
```

...

### Các biến số liên tục

...

```{r}
df%>%tidyr::gather(Age,BMI,AFC,Blasturation, 
                   key = "Feature", 
                   value = "Score")%>%
  ggplot()+
  geom_density(aes(x=Score,
                   fill=ClinPreg),
               alpha=0.5)+
  theme_bw(10)+
  scale_fill_manual(values=class_pals)+
  facet_wrap(~ Feature,
             ncol=2,
             scales = "free")
```

...

```{r}
binomial_smooth <- function(...) {
  geom_smooth(method = "glm", 
              formula = y ~ splines::bs(x,3),
              method.args = list(family = "binomial"), ...)
}

df%>%tidyr::gather(Age,BMI,AFC,Blasturation,
                   key = "Feature", 
                   value = "Score")%>%
  mutate(CP = as.numeric(ClinPreg)-1)%>%
  ggplot(aes(x=Score,
             y=CP)
         )+
  geom_point(aes(color = CP),
               alpha=0.3)+
  binomial_smooth(color = "black",
                  fill = "red",
                  alpha = 0.4)+
  theme_bw(10)+
  scale_color_gradient(low=class_pals[1],
                       high=class_pals[2])+
  facet_wrap(~ Feature,
             ncol=2,
             scales = "free")
```

...

```{r,cache =T}
plotfuncmid <- function(data,mapping){
  p <- ggplot(data = data,
              mapping=mapping)+
    geom_density(aes(fill=data$ClinPreg),
                 alpha=0.3,
                 color="black")+
    scale_fill_manual(values=class_pals)
  p
}

plotfuncLow <- function(data,mapping){
  p <- ggplot(data = data,
              mapping=mapping)+
    stat_density2d(geom="polygon",
                   aes(fill=data$ClinPreg,
                       alpha = ..level..))+
    scale_fill_manual(values=class_pals)+
    scale_color_manual(values=class_pals)
  p
}

plotfuncUp <- function(data,mapping){
  p <- ggplot(data = data,
              mapping=mapping)+
    geom_jitter(size = 0.05,
                alpha = 0.1,
                aes(color=data$ClinPreg))+
    geom_smooth(se=T,
                alpha=0.5,
                size = 0.5,
                aes(color=data$ClinPreg, 
                    fill = data$ClinPreg)
                )+
    scale_color_manual(values=class_pals)+
    scale_fill_manual(values=class_pals)
  p
}

library(GGally)

ggpairs(data = df,
        columns=c(2:4,10,12,14),
        lower=list(continuous=plotfuncLow),
        diag=list(continuous=plotfuncmid),
        upper = list(continuous=plotfuncUp))+
  theme_bw(5)
```

...

## Phân chia dữ liệu

...

```{r}
set.seed(12345)
init_split = df%>%initial_split(prop = 0.7)

train_data <- training(init_split)
test_data <- testing(init_split)
```

## Thiết lập mô hình phân loại

### Bước 3a) Công đoạn sơ chế dữ liệu

...

```{r}
train_recipe <- 
  recipe(ClinPreg ~ ., 
         data = train_data) %>% 
  step_dummy(all_nominal_predictors())
```

### Khởi tạo mô hình XGBoost

...

```{r}
xgboost_model <- boost_tree(
  mode = "classification",
  trees = 500,
  tree_depth = 5,
  sample_size = 0.8,
  learn_rate = 0.01,
  engine = "xgboost")
```

### Tạo workflow

...

```{r}
clinpreg_wf <- workflow() %>%
  add_recipe(train_recipe) %>%
  add_model(xgboost_model)
```

...

### Thực hiện kiểm định chéo lặp lại 10x10

...

## Các tiêu chí hiệu năng mô hình phân loại nhị phân

...

```{r, cache =T}
kfcv = vfold_cv(df, 
                v = 10,
                repeats = 10)

cv_res = clinpreg_wf%>%
  fit_resamples(resamples = kfcv, 
                metrics = metric_set(f_meas,
                                     bal_accuracy,
                                     sens, 
                                     spec,
                                     roc_auc
                                     ))
```

...

```{r}
cv_res_extract = cv_res$.metrics%>%
  bind_rows()

cv_res_extract %>% 
  group_by(.metric)%>%
  summarize(n = n(),
            mean = mean(.estimate),
            sd = sd(.estimate),
            median = median(.estimate),
            p5 = quantile(.estimate, 0.05),
            p95 = quantile(.estimate, 0.95)
            )%>%
    knitr::kable(digits = 3)
```

...

```{r}
cv_res_extract %>%
  ggplot(aes(x = .metric, y = .estimate))+
  geom_lv(aes(fill = ..LV..),
          col = 'black',
          show.legend = F)+
  coord_flip()+
  scale_fill_brewer(palette = "Reds", direction = -1)+
  theme_bw()
```

...

## Huấn luyện mô hình trên toàn thể dữ liệu tập Train

...

```{r, cache =T}
fit <- clinpreg_wf %>%
  fit(train_data)

fit_xgb = extract_fit_engine(fit)
```

...

```{r}
fit_xgb$evaluation_log %>%
  ggplot()+
  geom_path(aes(x = iter, y = training_logloss), 
            col = "red")+
  scale_x_continuous(breaks = seq(0,500,50))+
  scale_y_continuous(breaks = seq(0.2,0.8,0.025))+
  labs(x = "Iterations", y = "binary logloss")+
  theme_bw(10)
```

...

## Kiểm định độc lập hiệu năng mô hình

...

```{r, cache =T}
valid_pred_p = predict(fit, new_data = test_data, type ="prob")
valid_pred_c = predict(fit, new_data = test_data)

valid_out = tibble(truth = test_data$ClinPreg,
                   neg = valid_pred_p$.pred_0,
                   pos = valid_pred_p$.pred_1,
                   pred = valid_pred_c$.pred_class)

conf_mat(valid_out, 
         truth = truth, 
         estimate = pred)
```

```{r}
bind_rows(f_meas(valid_out,truth,pred),
          bal_accuracy(valid_out,truth,pred),
          sens(valid_out,truth,pred),
          spec(valid_out,truth,pred),
          precision(valid_out,truth,pred),
          npv(valid_out,truth,pred),
          ppv(valid_out,truth,pred),
          roc_auc(valid_out, 
                  truth, pos, 
                  event_level="second"))%>%
  knitr::kable(digits = 3)
```

...

```{r}
two_class_curve <- roc_curve(valid_out, 
                             truth, pos, 
                             event_level="second")

autoplot(two_class_curve)
```

...

## Phân tích hậu kiểm bằng kỹ thuật SHAP

...

### Giới thiệu về phương pháp diễn giải mô hình SHAP

...

```{r, cache =T}
log_mod = glm(formula = ClinPreg ~ .,
              data = train_data,
              family = "binomial")

tidy(log_mod)%>%
  knitr::kable(digits = 3)
```

...

## Vai trò của các thông số đóng góp vào kết quả tiên lượng

...

```{r, cache =T}
X_train <- bake(
  prep(train_recipe), 
  has_role("predictor"),
  new_data = train_data, 
  composition = "matrix"
)

X_test <- bake(
  prep(train_recipe), 
  has_role("predictor"),
  new_data = test_data, 
  composition = "matrix"
)
```

...

```{r, cache =T}
shap_data <- shap.prep(fit_xgb, 
                  X_train = X_train)

head(shap_data)
```

...

```{r, cache =T}
shap.plot.summary(shap_data)+
  scale_color_gradient2(low = "#078fe3",
                        mid = "#8007e3",
                        midpoint = 0.5,
                       high = "#e30750")
```

...

```{r, cache =T}
shap_data%>%ggplot(aes(x = reorder(variable, mean_value),
                  y = value))+
  geom_lv(aes(fill = mean_value),
               col = "black",
              alpha = 0.5)+
  scale_fill_gradient2(low="gold",
                       mid="red",
                       high="purple",
                       midpoint = 0.1)+
  scale_y_continuous(breaks = seq(-1,2,0.5))+
  labs(x = "Features", y = "SHAP value")+
  coord_flip()+
  theme_bw()
```

...

```{r, cache =T}
shap.importance(shap_data, 
                names_only = FALSE, 
                top_n = Inf)%>%
  ggplot(aes(x = reorder(variable,mean_abs_shap), 
             y = mean_abs_shap))+
  geom_bar(aes(fill = mean_abs_shap),
           stat="Identity",
           col = "black",
           alpha = 0.8,
           show.legend = F)+
  geom_text(aes(label=round(mean_abs_shap,3)), 
            vjust=0.15, 
            hjust=-0.15,
            size=3.5)+
  scale_y_continuous(limits = c(0,0.25))+
  labs(x = "Features")+
  coord_flip()+
  scale_fill_gradient2(low="gold",
                        mid="red",
                        high="purple",
                        midpoint = 0.1)+
  theme_bw()
```

...

```{r, cache =T}
shap_values <- shap.values(xgb_model = fit_xgb, 
                           X_train = X_train)
```


```{r, cache =T}
dpls = list()
# Step 4: Loop over dependence plots in decreasing importance
i=1
for (v in shap.importance(shap_data, names_only = TRUE)) {
  p = shap.plot.dependence(shap_data, 
                            v, color_feature = v, 
                            alpha = 0.5, 
                            jitter_width = 0.1,
                           size = 0.5)
  dpls[[i]] = p
  i = i+1
  print(p)
}
```

...

```{r, print = F}
# dpls[c(4,5,6,11)]
```

...

```{r,cache =T}
shap_int <- shap.prep.interaction(xgb_model = fit_xgb, 
                                  X_train = X_train)

shap.plot.dependence(data_long = shap_data,
                     data_int = shap_int,
                     x= "AFC", y = "Age", 
                     color_feature = "AFC")
```

...

### SHAP score có thể dùng để phân cụm dữ liệu

...

```{r,cache =T}
plot_data <- shap.prep.stack.data(shap_contrib = shap_values$shap_score, 
                                  top_n = 7, 
                                  n_groups = 4)

shap.plot.force_plot(plot_data, 
                     zoom_in_location = 500,
                     y_parent_limit = c(-0.1,0.1))

shap.plot.force_plot_bygroup(plot_data)
```

...

## Bàn luận: 

### Ưu điểm và nhược điểm của mô hình XGboost

...

### Ưu điểm và nhược điểm của kỹ thuật SHAP

...

### Diễn giải cơ chế hoạt động của mô hình ở cấp độ cá thể:

...

```{r, cache =T}
library(shapviz)

shaps <- shapviz(fit_xgb,
                X_pred = X_test)

sv_force(shaps, row_id = 150)
sv_waterfall(shaps, row_id = 150)
```

...

```{r,cache =T}
sv_force(shaps, row_id = 20)
sv_waterfall(shaps, row_id = 20)
```

...

## Kết luận

...

## Thông điệp rút gọn làm hành trang

...
